{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def as_bytes(num, final_size):\n",
    "    \"\"\"Converts an integer to a reversed bitstring (of size final_size).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num: int\n",
    "        The number to convert.\n",
    "    final_size: int\n",
    "        The length of the bitstring.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list:\n",
    "        A list which is the reversed bitstring representation of the given number.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> as_bytes(3, 4)\n",
    "    [1, 1, 0, 0]\n",
    "    >>> as_bytes(3, 5)\n",
    "    [1, 1, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for _ in range(final_size):\n",
    "        res.append(num % 2)\n",
    "        num //= 2\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_example(num_bits):\n",
    "    \"\"\"Generate an example addition.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_bits: int\n",
    "        The number of bits to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a: list\n",
    "        The first term (represented as reversed bitstring) of the addition.\n",
    "    b: list\n",
    "        The second term (represented as reversed bitstring) of the addition.\n",
    "    c: list\n",
    "        The addition (a + b) represented as reversed bitstring.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> np.random.seed(4)\n",
    "    >>> a, b, c = generate_example(3)\n",
    "    >>> a\n",
    "    [0, 1, 0]\n",
    "    >>> b\n",
    "    [0, 1, 0]\n",
    "    >>> c\n",
    "    [1, 0, 0]\n",
    "    >>> # Notice that these numbers are represented as reversed bitstrings)\n",
    "    \"\"\"\n",
    "    a = random.randint(0, 2 ** (num_bits - 1) - 1)\n",
    "    b = random.randint(0, 2 ** (num_bits - 1) - 1)\n",
    "    res = a + b\n",
    "    return (as_bytes(a, num_bits),\n",
    "            as_bytes(b, num_bits),\n",
    "            as_bytes(res, num_bits))\n",
    "\n",
    "\n",
    "def generate_batch(num_bits, batch_size):\n",
    "    \"\"\"Generates instances of the addition problem.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_bits: int\n",
    "        The number of bits to use for each number.\n",
    "    batch_size: int\n",
    "        The number of examples to generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: np.array\n",
    "        Two numbers to be added represented as bits (in reversed order).\n",
    "        Shape: b, i, n\n",
    "        Where:\n",
    "            b is bit index from the end.\n",
    "            i is example idx in batch.\n",
    "            n is one of [0,1] depending for first and second summand respectively.\n",
    "    y: np.array\n",
    "        The result of the addition.\n",
    "        Shape: b, i, n\n",
    "        Where:\n",
    "            b is bit index from the end.\n",
    "            i is example idx in batch.\n",
    "            n is always 0 since there is only one result.\n",
    "    \"\"\"\n",
    "    x = np.empty((batch_size, num_bits, 2))\n",
    "    y = np.empty((batch_size, num_bits, 1))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        a, b, r = generate_example(num_bits)\n",
    "        x[i, :, 0] = a\n",
    "        x[i, :, 1] = b\n",
    "        y[i, :, 0] = r\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Configuration\n",
    "batch_size = 100\n",
    "time_size = 5\n",
    "\n",
    "# Generate a test set and a train set containing 100 examples of numbers represented in 5 bits\n",
    "X_train, Y_train = generate_batch(time_size, batch_size)\n",
    "X_test, Y_test = generate_batch(time_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def as_bytes(num, final_size):\n",
    "    \"\"\"Converts an integer to a reversed bitstring (of size final_size).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num: int\n",
    "        The number to convert.\n",
    "    final_size: int\n",
    "        The length of the bitstring.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list:\n",
    "        A list which is the reversed bitstring representation of the given number.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> as_bytes(3, 4)\n",
    "    [1, 1, 0, 0]\n",
    "    >>> as_bytes(3, 5)\n",
    "    [1, 1, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for _ in range(final_size):\n",
    "        res.append(num % 2)\n",
    "        num //= 2\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_example(num_bits):\n",
    "    \"\"\"Generate an example addition.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_bits: int\n",
    "        The number of bits to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a: list\n",
    "        The first term (represented as reversed bitstring) of the addition.\n",
    "    b: list\n",
    "        The second term (represented as reversed bitstring) of the addition.\n",
    "    c: list\n",
    "        The addition (a + b) represented as reversed bitstring.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> np.random.seed(4)\n",
    "    >>> a, b, c = generate_example(3)\n",
    "    >>> a\n",
    "    [0, 1, 0]\n",
    "    >>> b\n",
    "    [0, 1, 0]\n",
    "    >>> c\n",
    "    [1, 0, 0]\n",
    "    >>> # Notice that these numbers are represented as reversed bitstrings)\n",
    "    \"\"\"\n",
    "    a = random.randint(0, 2 ** (num_bits - 1) - 1)\n",
    "    b = random.randint(0, 2 ** (num_bits - 1) - 1)\n",
    "    res = a + b\n",
    "    return (as_bytes(a, num_bits),\n",
    "            as_bytes(b, num_bits),\n",
    "            as_bytes(res, num_bits))\n",
    "\n",
    "\n",
    "def generate_batch(num_bits, batch_size):\n",
    "    \"\"\"Generates instances of the addition problem.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_bits: int\n",
    "        The number of bits to use for each number.\n",
    "    batch_size: int\n",
    "        The number of examples to generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: np.array\n",
    "        Two numbers to be added represented as bits (in reversed order).\n",
    "        Shape: b, i, n\n",
    "        Where:\n",
    "            b is bit index from the end.\n",
    "            i is example idx in batch.\n",
    "            n is one of [0,1] depending for first and second summand respectively.\n",
    "    y: np.array\n",
    "        The result of the addition.\n",
    "        Shape: b, i, n\n",
    "        Where:\n",
    "            b is bit index from the end.\n",
    "            i is example idx in batch.\n",
    "            n is always 0 since there is only one result.\n",
    "    \"\"\"\n",
    "    x = np.empty((batch_size, num_bits, 2))\n",
    "    y = np.empty((batch_size, num_bits, 1))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        a, b, r = generate_example(num_bits)\n",
    "        x[i, :, 0] = a\n",
    "        x[i, :, 1] = b\n",
    "        y[i, :, 0] = r\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Configuration\n",
    "batch_size = 100\n",
    "time_size = 5\n",
    "\n",
    "# Generate a test set and a train set containing 100 examples of numbers represented in 5 bits\n",
    "X_train, Y_train = generate_batch(time_size, batch_size)\n",
    "X_test, Y_test = generate_batch(time_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GRU:\n",
    "    \"\"\"Implementation of a Gated Recurrent Unit (GRU) as described in [1].\n",
    "\n",
    "    [1] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.  [Titel anhand dieser ArXiv-ID in Citavi-Projekt übernehmen]\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    input_dimensions: int\n",
    "        The size of the input vectors (x_t).\n",
    "    hidden_size: int\n",
    "        The size of the hidden layer vectors (h_t).\n",
    "    dtype: obj\n",
    "        The datatype used for the variables and constants (optional).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "\n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "\n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "\n",
    "        # Define the input layer placeholder\n",
    "        self.input_layer = tf.compat.v1.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
    "\n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
    "\n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "\n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
    "\n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "\n",
    "    def forward_pass(self, h_tm1, x_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        # Definitions of z_t and r_t\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "\n",
    "        # Definition of h~_t\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "\n",
    "        # Compute the next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
    "\n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The input has 2 dimensions: dimension 0 is reserved for the first term and dimension 1 is reverved for the second term\n",
    "input_dimensions = 2\n",
    "\n",
    "# Arbitrary number for the size of the hidden state\n",
    "hidden_size = 16\n",
    "\n",
    "# Initialize a session\n",
    "session = tf.compat.v1.Session()\n",
    "\n",
    "# Create a new instance of the GRU model\n",
    "gru = GRU(input_dimensions, hidden_size)\n",
    "\n",
    "# Add an additional layer on top of each of the hidden state outputs\n",
    "W_output = tf.Variable(tf.random.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "b_output = tf.Variable(tf.random.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "# Create a placeholder for the expected output\n",
    "expected_output = tf.compat.v1.placeholder(dtype=tf.float64, shape=(batch_size, time_size, 1), name='expected_output')\n",
    "\n",
    "# Just use quadratic loss\n",
    "loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output, 2)) / float(batch_size)\n",
    "\n",
    "# Use the Adam optimizer for training\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "#train_step = tf.optimizers.Adam().minimize(loss, var_list=[W_output,b_output], tape=tf.GradientTape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, train loss: 1.2855, test loss: 1.1006\n",
      "Iteration: 50, train loss: 0.5951, test loss: 0.6320\n",
      "Iteration: 100, train loss: 0.5320, test loss: 0.6105\n",
      "Iteration: 150, train loss: 0.3273, test loss: 0.4351\n",
      "Iteration: 200, train loss: 0.1588, test loss: 0.1771\n",
      "Iteration: 250, train loss: 0.0787, test loss: 0.0867\n",
      "Iteration: 300, train loss: 0.0260, test loss: 0.0341\n",
      "Iteration: 350, train loss: 0.0067, test loss: 0.0108\n",
      "Iteration: 400, train loss: 0.0021, test loss: 0.0048\n",
      "Iteration: 450, train loss: 0.0014, test loss: 0.0036\n",
      "Iteration: 500, train loss: 0.0009, test loss: 0.0027\n",
      "Iteration: 550, train loss: 0.0007, test loss: 0.0022\n",
      "Iteration: 600, train loss: 0.0005, test loss: 0.0019\n",
      "Iteration: 650, train loss: 0.0004, test loss: 0.0017\n",
      "Iteration: 700, train loss: 0.0003, test loss: 0.0016\n",
      "Iteration: 750, train loss: 0.0003, test loss: 0.0015\n",
      "Iteration: 800, train loss: 0.0002, test loss: 0.0014\n",
      "Iteration: 850, train loss: 0.0002, test loss: 0.0013\n",
      "Iteration: 900, train loss: 0.0002, test loss: 0.0013\n",
      "Iteration: 950, train loss: 0.0002, test loss: 0.0012\n",
      "Iteration: 1000, train loss: 0.0001, test loss: 0.0011\n",
      "Iteration: 1050, train loss: 0.0001, test loss: 0.0011\n",
      "Iteration: 1100, train loss: 0.0001, test loss: 0.0010\n",
      "Iteration: 1150, train loss: 0.0011, test loss: 0.0019\n",
      "Iteration: 1200, train loss: 0.0001, test loss: 0.0010\n",
      "Iteration: 1250, train loss: 0.0001, test loss: 0.0009\n",
      "Iteration: 1300, train loss: 0.0001, test loss: 0.0009\n",
      "Iteration: 1350, train loss: 0.0001, test loss: 0.0009\n",
      "Iteration: 1400, train loss: 0.0001, test loss: 0.0009\n",
      "Iteration: 1450, train loss: 0.0002, test loss: 0.0010\n",
      "Iteration: 1500, train loss: 0.0000, test loss: 0.0009\n",
      "Iteration: 1550, train loss: 0.0000, test loss: 0.0008\n",
      "Iteration: 1600, train loss: 0.0000, test loss: 0.0008\n",
      "Iteration: 1650, train loss: 0.0000, test loss: 0.0008\n",
      "Iteration: 1700, train loss: 0.0000, test loss: 0.0008\n",
      "Iteration: 1750, train loss: 0.0000, test loss: 0.0008\n",
      "Iteration: 1800, train loss: 0.0001, test loss: 0.0008\n",
      "Iteration: 1850, train loss: 0.0000, test loss: 0.0008\n",
      "Iteration: 1900, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 1950, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2000, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2050, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2100, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2150, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2200, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2250, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2300, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2350, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2400, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2450, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2500, train loss: 0.0000, test loss: 0.0007\n",
      "Iteration: 2550, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 2600, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 2650, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 2700, train loss: 0.0001, test loss: 0.0006\n",
      "Iteration: 2750, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 2800, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 2850, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 2900, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 2950, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3000, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3050, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3100, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3150, train loss: 0.0016, test loss: 0.0034\n",
      "Iteration: 3200, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3250, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3300, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3350, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3400, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3450, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3500, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3550, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3600, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3650, train loss: 0.0036, test loss: 0.0041\n",
      "Iteration: 3700, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3750, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3800, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3850, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3900, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 3950, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4000, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4050, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4100, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4150, train loss: 0.0007, test loss: 0.0009\n",
      "Iteration: 4200, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4250, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4300, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4350, train loss: 0.0001, test loss: 0.0006\n",
      "Iteration: 4400, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4450, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4500, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4550, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4600, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4650, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4700, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4750, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4800, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4850, train loss: 0.0000, test loss: 0.0006\n",
      "Iteration: 4900, train loss: 0.0000, test loss: 0.0005\n",
      "Iteration: 4950, train loss: 0.0000, test loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize all the variables\n",
    "init_variables = tf.compat.v1.global_variables_initializer()\n",
    "session.run(init_variables)\n",
    "\n",
    "# Initialize the losses\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Perform all the iterations\n",
    "for epoch in range(5000):\n",
    "    # Compute the losses\n",
    "    _, train_loss = session.run([train_step, loss], feed_dict={gru.input_layer: X_train, expected_output: Y_train})\n",
    "    validation_loss = session.run(loss, feed_dict={gru.input_layer: X_test, expected_output: Y_test})\n",
    "\n",
    "    # Log the losses\n",
    "    train_losses += [train_loss]\n",
    "    validation_losses += [validation_loss]\n",
    "\n",
    "    # Display an update every 50 iterations\n",
    "    if epoch % 50 == 0:\n",
    "        #plt.plot(train_losses, '-b', label='Train loss')\n",
    "        #plt.plot(validation_losses, '-r', label='Validation loss')\n",
    "        #plt.legend(loc=0)\n",
    "        #plt.title('Loss')\n",
    "        #plt.xlabel('Iteration')\n",
    "        #plt.ylabel('Loss')\n",
    "        #plt.show()\n",
    "        print('Iteration: %d, train loss: %.4f, test loss: %.4f' % (epoch, train_loss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define two numbers a and b and let the model compute a + b\n",
    "a = 1024\n",
    "b = 16\n",
    "\n",
    "# The model is independent of the sequence length! Now we can test the model on even longer bitstrings\n",
    "bitstring_length = 20\n",
    "\n",
    "# Create the feature vectors\n",
    "X_custom_sample = np.vstack([as_bytes(a, bitstring_length), as_bytes(b, bitstring_length)]).T\n",
    "X_custom = np.zeros((1,) + X_custom_sample.shape)\n",
    "X_custom[0, :, :] = X_custom_sample\n",
    "\n",
    "# Make a prediction by using the model\n",
    "y_predicted = session.run(output, feed_dict={gru.input_layer: X_custom})\n",
    "# Just use a linear class separator at 0.5\n",
    "y_bits = 1 * (y_predicted > 0.5)[0, :, 0]\n",
    "# Join and reverse the bitstring\n",
    "y_bitstr = ''.join([str(int(bit)) for bit in y_bits.tolist()])[::-1]\n",
    "# Convert the found bitstring to a number\n",
    "y = int(y_bitstr, 2)\n",
    "\n",
    "# Print out the prediction\n",
    "print(y)  # Yay! This should equal 1024 + 16 = 1040"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
