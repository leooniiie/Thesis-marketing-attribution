{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Working GRU Network for Byte addition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "%matplotlib inline\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def as_bytes(num, final_size):\n",
    "    \"\"\"Converts an integer to a reversed bitstring (of size final_size).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num: int\n",
    "        The number to convert.\n",
    "    final_size: int\n",
    "        The length of the bitstring.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list:\n",
    "        A list which is the reversed bitstring representation of the given number.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> as_bytes(3, 4)\n",
    "    [1, 1, 0, 0]\n",
    "    >>> as_bytes(3, 5)\n",
    "    [1, 1, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for _ in range(final_size):\n",
    "        res.append(num % 2)\n",
    "        num //= 2\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_example(num_bits):\n",
    "    \"\"\"Generate an example addition.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_bits: int\n",
    "        The number of bits to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a: list\n",
    "        The first term (represented as reversed bitstring) of the addition.\n",
    "    b: list\n",
    "        The second term (represented as reversed bitstring) of the addition.\n",
    "    c: list\n",
    "        The addition (a + b) represented as reversed bitstring.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> np.random.seed(4)\n",
    "    >>> a, b, c = generate_example(3)\n",
    "    >>> a\n",
    "    [0, 1, 0]\n",
    "    >>> b\n",
    "    [0, 1, 0]\n",
    "    >>> c\n",
    "    [1, 0, 0]\n",
    "    >>> # Notice that these numbers are represented as reversed bitstrings)\n",
    "    \"\"\"\n",
    "    a = random.randint(0, 2 ** (num_bits - 1) - 1)\n",
    "    b = random.randint(0, 2 ** (num_bits - 1) - 1)\n",
    "    res = a + b\n",
    "    return (as_bytes(a, num_bits),\n",
    "            as_bytes(b, num_bits),\n",
    "            as_bytes(res, num_bits))\n",
    "\n",
    "\n",
    "def generate_batch(num_bits, batch_size):\n",
    "    \"\"\"Generates instances of the addition problem.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_bits: int\n",
    "        The number of bits to use for each number.\n",
    "    batch_size: int\n",
    "        The number of examples to generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: np.array\n",
    "        Two numbers to be added represented as bits (in reversed order).\n",
    "        Shape: b, i, n\n",
    "        Where:\n",
    "            b is bit index from the end.\n",
    "            i is example idx in batch.\n",
    "            n is one of [0,1] depending for first and second summand respectively.\n",
    "    y: np.array\n",
    "        The result of the addition.\n",
    "        Shape: b, i, n\n",
    "        Where:\n",
    "            b is bit index from the end.\n",
    "            i is example idx in batch.\n",
    "            n is always 0 since there is only one result.\n",
    "    \"\"\"\n",
    "    x = np.empty((batch_size, num_bits, 2))\n",
    "    y = np.empty((batch_size, num_bits, 1))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        a, b, r = generate_example(num_bits)\n",
    "        x[i, :, 0] = a\n",
    "        x[i, :, 1] = b\n",
    "        y[i, :, 0] = r\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Configuration\n",
    "batch_size = 100\n",
    "time_size = 5\n",
    "\n",
    "# Generate a test set and a train set containing 100 examples of numbers represented in 5 bits\n",
    "X_train, Y_train = generate_batch(time_size, batch_size)\n",
    "X_test, Y_test = generate_batch(time_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.convert_to_tensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GRU:\n",
    "    \"\"\"Implementation of a Gated Recurrent Unit (GRU) as described in [1].\n",
    "\n",
    "    [1] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.  [Titel anhand dieser ArXiv-ID in Citavi-Projekt Ã¼bernehmen]\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    input_dimensions: int\n",
    "        The size of the input vectors (x_t).\n",
    "    hidden_size: int\n",
    "        The size of the hidden layer vectors (h_t).\n",
    "    dtype: obj\n",
    "        The datatype used for the variables and constants (optional).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
    "        self.Wr = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wr')\n",
    "        self.Wz = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wz')\n",
    "        self.Wh = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.input_dimensions, self.hidden_size), mean=0, stddev=0.01), name='Wh')\n",
    "\n",
    "        # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
    "        self.Ur = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Ur')\n",
    "        self.Uz = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uz')\n",
    "        self.Uh = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size, self.hidden_size), mean=0, stddev=0.01), name='Uh')\n",
    "\n",
    "        # Biases for hidden vectors of shape (hidden_size,)\n",
    "        self.br = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='br')\n",
    "        self.bz = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bz')\n",
    "        self.bh = tf.Variable(tf.random.truncated_normal(dtype=dtype, shape=(self.hidden_size,), mean=0, stddev=0.01), name='bh')\n",
    "\n",
    "        # Define the input layer placeholder\n",
    "        self.input_layer = tf.compat.v1.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
    "\n",
    "        # Put the time-dimension upfront for the scan operator\n",
    "        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
    "\n",
    "        # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
    "        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
    "\n",
    "        # Perform the scan operator\n",
    "        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
    "\n",
    "        # Transpose the result back\n",
    "        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
    "\n",
    "    def forward_pass(self, h_tm1, x_t):\n",
    "        \"\"\"Perform a forward pass.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        h_tm1: np.matrix\n",
    "            The hidden state at the previous timestep (h_{t-1}).\n",
    "        x_t: np.matrix\n",
    "            The input vector.\n",
    "        \"\"\"\n",
    "        # Definitions of z_t and r_t\n",
    "        z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
    "        r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
    "\n",
    "        # Definition of h~_t\n",
    "        h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
    "\n",
    "        # Compute the next hidden state\n",
    "        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
    "\n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The input has 2 dimensions: dimension 0 is reserved for the first term and dimension 1 is reverved for the second term\n",
    "input_dimensions = 2\n",
    "\n",
    "# Arbitrary number for the size of the hidden state\n",
    "hidden_size = 16\n",
    "\n",
    "# Initialize a session\n",
    "session = tf.compat.v1.Session()\n",
    "\n",
    "# Create a new instance of the GRU model\n",
    "gru = GRU(input_dimensions, hidden_size)\n",
    "\n",
    "# Add an additional layer on top of each of the hidden state outputs\n",
    "W_output = tf.Variable(tf.random.truncated_normal(dtype=tf.float64, shape=(hidden_size, 1), mean=0, stddev=0.01))\n",
    "b_output = tf.Variable(tf.random.truncated_normal(dtype=tf.float64, shape=(1,), mean=0, stddev=0.01))\n",
    "output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
    "\n",
    "# Create a placeholder for the expected output\n",
    "expected_output = tf.compat.v1.placeholder(dtype=tf.float64, shape=(batch_size, time_size, 1), name='expected_output')\n",
    "\n",
    "# Just use quadratic loss\n",
    "loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output, 2)) / float(batch_size)\n",
    "\n",
    "# Use the Adam optimizer for training\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "#train_step = tf.optimizers.Adam().minimize(loss, var_list=[W_output,b_output], tape=tf.GradientTape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, train loss: 1.2455, test loss: 1.1280\n",
      "Iteration: 50, train loss: 0.6138, test loss: 0.6085\n",
      "Iteration: 100, train loss: 0.5630, test loss: 0.5869\n",
      "Iteration: 150, train loss: 0.4792, test loss: 0.4980\n",
      "Iteration: 200, train loss: 0.1335, test loss: 0.1823\n",
      "Iteration: 250, train loss: 0.0536, test loss: 0.0869\n",
      "Iteration: 300, train loss: 0.0167, test loss: 0.0252\n",
      "Iteration: 350, train loss: 0.0013, test loss: 0.0019\n",
      "Iteration: 400, train loss: 0.0005, test loss: 0.0008\n",
      "Iteration: 450, train loss: 0.0003, test loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize all the variables\n",
    "init_variables = tf.compat.v1.global_variables_initializer()\n",
    "session.run(init_variables)\n",
    "\n",
    "# Initialize the losses\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "ep = []\n",
    "epochs = 500\n",
    "\n",
    "# Perform all the iterations\n",
    "for epoch in range(epochs):\n",
    "    # Compute the losses\n",
    "    _, train_loss = session.run([train_step, loss], feed_dict={gru.input_layer: X_train, expected_output: Y_train})\n",
    "    validation_loss = session.run(loss, feed_dict={gru.input_layer: X_test, expected_output: Y_test})\n",
    "\n",
    "    # Log the losses\n",
    "    #train_losses += [train_loss]\n",
    "    #validation_losses += [validation_loss]\n",
    "\n",
    "    # Display an update every 50 iterations\n",
    "    if epoch % 50 == 0:\n",
    "        ep += [epoch]\n",
    "        train_losses += [train_loss]\n",
    "        validation_losses += [validation_loss]\n",
    "        #plt.plot(train_losses, '-b', label='Train loss')\n",
    "        #plt.plot(validation_losses, '-r', label='Validation loss')\n",
    "        #plt.legend(loc=0)\n",
    "        #plt.title('Loss')\n",
    "        #plt.xlabel('Iteration')\n",
    "        #plt.ylabel('Loss')\n",
    "        #plt.show()\n",
    "        print('Iteration: %d, train loss: %.4f, test loss: %.4f' % (epoch, train_loss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0      0\n1     50\n2    100\n3    150\n4    200\n5    250\n6    300\n7    350\n8    400\n9    450\nName: epoch, dtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses= {'epoch': ep,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': validation_losses\n",
    "        }\n",
    "losses = pd.DataFrame(losses)\n",
    "losses['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 50, 100, 150, 200, 250, 300, 350, 400, 450]\n",
      "[1.2454925822048386, 0.6138228325280721, 0.5630025727620003, 0.47919945641624134, 0.13346553067774333, 0.05357701337052395, 0.016748065598634124, 0.0013292650277522105, 0.0005290772376396369, 0.00033173130483851086]\n"
     ]
    }
   ],
   "source": [
    "print(ep)\n",
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laptop is to lame to do this\n",
    "#plt.plot(ep, train_losses, 'r', ep, validation_losses, 'b')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-5.49073333e-04]\n",
      "  [ 9.22734898e-03]\n",
      "  [ 2.87649697e-03]\n",
      "  [ 2.45596749e-03]\n",
      "  [ 1.00799765e+00]\n",
      "  [-8.96696514e-03]\n",
      "  [-4.35089031e-04]\n",
      "  [-9.52395668e-03]\n",
      "  [-4.02547075e-03]\n",
      "  [-2.16741495e-03]\n",
      "  [ 1.00033866e+00]\n",
      "  [-3.83914268e-03]\n",
      "  [-1.21384661e-03]\n",
      "  [-9.79491772e-03]\n",
      "  [-4.13137641e-03]\n",
      "  [-2.20966667e-03]\n",
      "  [-1.18989125e-03]\n",
      "  [-7.14454835e-04]\n",
      "  [-4.90788299e-04]\n",
      "  [-3.86465470e-04]]]\n",
      "1040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define two numbers a and b and let the model compute a + b\n",
    "a = 1024\n",
    "b = 16\n",
    "\n",
    "# The model is independent of the sequence length! Now we can test the model on even longer bitstrings\n",
    "bitstring_length = 20\n",
    "\n",
    "# Create the feature vectors\n",
    "X_custom_sample = np.vstack([as_bytes(a, bitstring_length), as_bytes(b, bitstring_length)]).T\n",
    "X_custom = np.zeros((1,) + X_custom_sample.shape)\n",
    "X_custom[0, :, :] = X_custom_sample\n",
    "\n",
    "# Make a prediction by using the model\n",
    "y_predicted = session.run(output, feed_dict={gru.input_layer: X_custom})\n",
    "\n",
    "# Just use a linear class separator at 0.5\n",
    "y_bits = 1 * (y_predicted > 0.5)[0, :, 0]\n",
    "# Join and reverse the bitstring\n",
    "y_bitstr = ''.join([str(int(bit)) for bit in y_bits.tolist()])[::-1]\n",
    "# Convert the found bitstring to a number\n",
    "y = int(y_bitstr, 2)\n",
    "\n",
    "# Print out the prediction\n",
    "print(y)  # Yay! This should equal 1024 + 16 = 1040"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
